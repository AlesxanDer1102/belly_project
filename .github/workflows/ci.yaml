name: CI Pipeline Python

on:
  push:
    branches: [main, master, develop, "feature/*"]
  pull_request:
    branches: [main, master]

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest behave
          pip install -r requirements.txt

      - name: Run pytest
        id: pytest
        continue-on-error: true
        run: |
          pytest -v --strict-markers --junitxml=test-reports/pytest-report.xml

      - name: Run behave (español)
        id: behave_es
        continue-on-error: true
        run: |
          behave --junit --junit-directory=test-reports --tags=spanish features/ -v

      - name: Run behave (inglés)
        id: behave_en
        continue-on-error: true
        run: |
          behave --junit --junit-directory=test-reports --tags=english features/ -v

      - name: Run behave (tiempos aleatorios)
        id: behave_random
        continue-on-error: true
        run: |
          export PYTHONHASHSEED=42 
          behave --junit --junit-directory=test-reports --tags=random_time features/ -v

      - name: Run behave (pruebas de validación)
        id: behave_validacion
        continue-on-error: true
        run: |
          behave --junit --junit-directory=test-reports --tags=validacion features/ -v

      - name: Run behave (pruebas de escalabilidad)
        id: behave_escalabilidad
        continue-on-error: true
        run: |
          /usr/bin/time -v behave --junit --junit-directory=test-reports --tags=stress_test features/ -v

      - name: Run behave (todas las pruebas)
        id: behave_all
        continue-on-error: true
        run: |
          behave --junit --junit-directory=test-reports features/ -v

      - name: Verificar manejo de excepciones
        continue-on-error: true
        id: verify_exceptions
        run: |
          echo "Verificando manejo de excepciones..."

          if grep -q "Error por cantidad negativa detectado correctamente" test-reports/TESTS-*.xml; then
            echo "✅ Validación de cantidades negativas: OK"
          else
            echo "⚠️ Validación de cantidades negativas: No detectada"
            VALIDATION_ERRORS=true
          fi

          if grep -q "Error por cantidad excesiva detectado correctamente" test-reports/TESTS-*.xml; then
            echo "✅ Validación de cantidades excesivas: OK"
          else
            echo "⚠️ Validación de cantidades excesivas: No detectada"
            VALIDATION_ERRORS=true
          fi

          if [ "$VALIDATION_ERRORS" = true ]; then
            echo "::warning::Algunas validaciones no se detectaron correctamente"
          fi

      - name: Verificar pruebas de rendimiento
        continue-on-error: true
        id: verify_performance
        run: |
          echo "Verificando pruebas de rendimiento..."

          if grep -q "Tiempo de ejecución:" test-reports/TESTS-*.xml; then
            echo "✅ Medición de rendimiento: OK"
          else
            echo "⚠️ Medición de rendimiento: No detectada"
          fi

          # Verificar que ninguna prueba haya excedido el límite de tiempo
          if grep -q "más de los .* segundos permitidos" test-reports/TESTS-*.xml; then
            echo "❌ Algunas pruebas excedieron el límite de tiempo"
            PERFORMANCE_ERRORS=true
          else
            echo "✅ Todas las pruebas dentro de los límites de tiempo"
          fi

          if [ "$PERFORMANCE_ERRORS" = true ]; then
            echo "::warning::Problemas de rendimiento detectados"
          fi

      - name: Generate test reports
        if: always()
        run: |
          mkdir -p test-reports

          # Generar un reporte resumen
          echo "# Resumen de ejecución de pruebas" > test-reports/summary.md
          echo "Fecha: $(date)" >> test-reports/summary.md
          echo "" >> test-reports/summary.md
          echo "## Resultados" >> test-reports/summary.md
          echo "- pytest: ${{ steps.pytest.outcome }}" >> test-reports/summary.md
          echo "- behave (español): ${{ steps.behave_es.outcome }}" >> test-reports/summary.md
          echo "- behave (inglés): ${{ steps.behave_en.outcome }}" >> test-reports/summary.md
          echo "- behave (tiempos aleatorios): ${{ steps.behave_random.outcome }}" >> test-reports/summary.md
          echo "- behave (validación): ${{ steps.behave_validacion.outcome }}" >> test-reports/summary.md
          echo "- behave (escalabilidad): ${{ steps.behave_escalabilidad.outcome }}" >> test-reports/summary.md
          echo "- behave (todas): ${{ steps.behave_all.outcome }}" >> test-reports/summary.md

      - name: Check test results
        if: always()
        run: |
          echo "========= RESULTADOS DE LAS PRUEBAS ========="
          echo "- pytest: ${{ steps.pytest.outcome }}"
          echo "- behave (español): ${{ steps.behave_es.outcome }}"
          echo "- behave (inglés): ${{ steps.behave_en.outcome }}"
          echo "- behave (tiempos aleatorios): ${{ steps.behave_random.outcome }}"
          echo "- behave (validación): ${{ steps.behave_validacion.outcome }}"
          echo "- behave (escalabilidad): ${{ steps.behave_escalabilidad.outcome }}"
          echo "- behave (todas): ${{ steps.behave_all.outcome }}"

          # Definir las pruebas críticas que no pueden fallar
          CRITICAL_TESTS="${{ steps.pytest.outcome }} ${{ steps.behave_es.outcome }} ${{ steps.behave_en.outcome }}"

          # Fallar el build solo si fallan las pruebas críticas
          if [[ "$CRITICAL_TESTS" == *"failure"* ]]; then
            echo "::error::❌ Pruebas críticas han fallado!"
            exit 1
          elif [[ "${{ steps.behave_all.outcome }}" == "failure" ]]; then
            echo "::warning::⚠️ Algunas pruebas han fallado pero no son críticas"
          else
            echo "::notice::✅ Todas las pruebas han pasado correctamente"
          fi

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results
          path: test-reports/
